{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\AI_A\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchmetrics import Accuracy\n",
    "from init import seed_everything, dump_args, data_preprocess, test_preprocess\n",
    "from dataset import CharDataset\n",
    "from models import LeNet, ResNet18\n",
    "from classifier import CNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着使用 `args` 存储相关的参数，同时使用 `dump_args` 导出本次运行的参数到文件，如果要保存日志文件，须设置参数 `log=True` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 17:52:47,199 P20936 INFO Dump args: {\n",
      "    \"model\": \"ResNet18\",\n",
      "    \"epoches\": 100,\n",
      "    \"batch_size\": 32,\n",
      "    \"learning_rate\": 0.002,\n",
      "    \"record_path\": \"./record\",\n",
      "    \"save_path\": \"./save\\\\ResNet18_best.ckpt\",\n",
      "    \"raw_data_path\": \"./data/raw\",\n",
      "    \"data_path\": \"./data/data.npz\",\n",
      "    \"random_seed\": 42,\n",
      "    \"mode\": \"train_and_test\",\n",
      "    \"hash_id\": \"a4db3cb6\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "args = {}\n",
    "args[\"model\"] = \"ResNet18\"                      # [\"LeNet\", \"ResNet18\"]\n",
    "args[\"epoches\"] = 100\n",
    "args[\"batch_size\"] = 32\n",
    "args[\"learning_rate\"] = 0.03\n",
    "args[\"record_path\"] = \"./record\"\n",
    "args[\"save_path\"] = \"./save\"                 # 完整路径是 f\"{args['save_path']}/{args['model']}_best.ckpt\"\n",
    "args[\"raw_data_path\"] = \"./data/raw\"\n",
    "args[\"data_path\"] = \"./data/data.npz\"\n",
    "args[\"random_seed\"] = 42\n",
    "args[\"mode\"] = \"train_and_test\"              # [\"train\", \"test\", \"train_and_test\"]\n",
    "\n",
    "args[\"save_path\"] = os.path.join(args[\"save_path\"], f\"{args['model']}_best.ckpt\")\n",
    "record_path = dump_args(args, args[\"record_path\"], True, args[\"mode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着使用 `args` 存储相关的参数，同时使用 `dump_args` 导出本次运行的参数到文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试加载训练数据，如果训练数据不存在，则通过 `data_preprocess` 生成。加载数据后初始化训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args[\"data_path\"]):\n",
    "    data_preprocess(args[\"raw_data_path\"], args[\"data_path\"])\n",
    "data_npz = np.load(args[\"data_path\"])\n",
    "\n",
    "if args[\"model\"] == \"LeNet\":\n",
    "    transform = Compose([ToTensor(), Resize((32,32), antialias=False)])\n",
    "elif args[\"model\"] == \"ResNet18\":\n",
    "    transform = ToTensor()\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "dataset_train = CharDataset(data_npz[\"x_train\"], data_npz[\"y_train\"], transform)\n",
    "dataloader_train = DataLoader(dataset_train, args[\"batch_size\"], shuffle=True)\n",
    "\n",
    "dataset_valid = CharDataset(data_npz[\"x_valid\"], data_npz[\"y_valid\"], transform)\n",
    "dataloader_valid = DataLoader(dataset_valid, args[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"model\"] == \"LeNet\":\n",
    "    model = LeNet(1, 12)\n",
    "elif args[\"model\"] == \"ResNet18\":\n",
    "    model = ResNet18(1, 12)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "classifier = CNNClassifier(model, gpu=0)\n",
    "if os.path.exists(args[\"save_path\"]):\n",
    "    classifier.load_model(args[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 17:53:08,819 P20936 INFO Train epoch 1/100, learning rate: 0.002, train loss: 0.540855, train_acc: 0.830496 [10.39s]\n",
      "2023-10-15 17:53:09,019 P20936 INFO Best validation accuracy has been updated: 0.000000 -> 0.907258\n",
      "2023-10-15 17:53:09,086 P20936 INFO Save model to ./save\\ResNet18_best.ckpt\n",
      "2023-10-15 17:53:15,468 P20936 INFO Train epoch 2/100, learning rate: 0.002, train loss: 2.916418, train_acc: 0.089456 [6.38s]\n",
      "2023-10-15 17:53:22,073 P20936 INFO Train epoch 3/100, learning rate: 0.002, train loss: 2.485564, train_acc: 0.081392 [6.41s]\n",
      "2023-10-15 17:53:28,689 P20936 INFO Train epoch 4/100, learning rate: 0.002, train loss: 2.485362, train_acc: 0.084379 [6.41s]\n",
      "2023-10-15 17:53:35,277 P20936 INFO Train epoch 5/100, learning rate: 0.002, train loss: 2.485239, train_acc: 0.079002 [6.39s]\n",
      "2023-10-15 17:53:41,909 P20936 INFO Train epoch 6/100, learning rate: 0.002, train loss: 2.485341, train_acc: 0.085723 [6.43s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_and_test\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AI_A\\Project1\\Part2\\classifier.py:46\u001b[0m, in \u001b[0;36mCNNClassifier.fit\u001b[1;34m(self, train_loader, valid_loader, epoches, learning_rate, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/AI_A/Project1/Part2/classifier.py?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='file:///d%3A/AI_A/Project1/Part2/classifier.py?line=44'>45</a>\u001b[0m     x, y \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> <a href='file:///d%3A/AI_A/Project1/Part2/classifier.py?line=45'>46</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     <a href='file:///d%3A/AI_A/Project1/Part2/classifier.py?line=46'>47</a>\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='file:///d%3A/AI_A/Project1/Part2/classifier.py?line=47'>48</a>\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args[\"mode\"] == \"train\" or args[\"mode\"] == \"train_and_test\":\n",
    "    classifier.fit(\n",
    "        train_loader=dataloader_train,\n",
    "        valid_loader=dataloader_valid,\n",
    "        epoches=args[\"epoches\"],\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        save_path=args[\"save_path\"],\n",
    "        log_interval=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"mode\"] != \"test\" and args[\"mode\"] != \"train_and_test\":\n",
    "    exit(0)\n",
    "\n",
    "raw_test_path = \"\"\n",
    "test_path = \"\"\n",
    "if not os.path.exists(test_path):\n",
    "    test_preprocess(raw_test_path, test_path, shuffle=True)\n",
    "test_npz = np.load(test_path)\n",
    "\n",
    "dataset_test = CharDataset(test_npz[\"x_test\"], test_npz[\"y_test\"], transform)\n",
    "dataloader_test = DataLoader(dataset_test, args[\"batch_size\"], shuffle=False)\n",
    "\n",
    "device = classifier.device\n",
    "\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "loss_fn = F.cross_entropy\n",
    "metric = Accuracy(task=\"multiclass\", num_classes=12, top_k=1).to(device)\n",
    "for batch in dataloader_test:\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.long().to(device)\n",
    "    preds = classifier.predict(x)\n",
    "    loss = loss_fn(preds, y)\n",
    "    test_loss += loss.item()\n",
    "    metric.update(preds, y)\n",
    "test_loss /= len(dataloader_test)\n",
    "test_acc = metric.compute()\n",
    "logging.info(\"Test mean loss: {:.6f}, mean accuracy: {:.6f}\".format(test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73f83993ecc2dce571ce89cddd8a44e114591bb14d9f1f8465ac4f80026585cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('AI_A')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
