{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from init import seed_everything, dump_args, data_preprocess\n",
    "from dataset import CharDataset\n",
    "from models import LeNet, ResNet\n",
    "from classifier import CNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着使用 `args` 存储相关的参数，同时使用 `dump_args` 导出本次运行的参数到文件，如果要保存日志文件，须设置参数 `log=True` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"model\"] = \"LeNet\"                      # [\"LeNet\", \"ResNet\"]\n",
    "args[\"epoches\"] = 100\n",
    "args[\"batch_size\"] = 8\n",
    "args[\"learning_rate\"] = 0.0005\n",
    "args[\"record_path\"] = \"./record\"\n",
    "args[\"save_path\"] = \"./save\"                 # 完整路径是 f\"{args['save_path']}/{args['model']}_best.ckpt\"\n",
    "args[\"raw_data_path\"] = \"./data/raw\"\n",
    "args[\"data_path\"] = \"./data/data.npz\"\n",
    "args[\"random_seed\"] = 42\n",
    "args[\"mode\"] = \"train_and_test\"              # [\"train\", \"test\", \"train_and_test\"]\n",
    "\n",
    "args[\"save_path\"] = os.path.join(args[\"save_path\"], f\"{args['model']}_best.ckpt\")\n",
    "record_path = dump_args(args, args[\"record_path\"], True, args[\"mode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着使用 `args` 存储相关的参数，同时使用 `dump_args` 导出本次运行的参数到文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(args[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试加载训练数据，如果训练数据不存在，则通过 `data_preprocess` 生成。加载数据后初始化训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args[\"data_path\"]):\n",
    "    data_preprocess(args[\"raw_data_path\"], args[\"data_path\"])\n",
    "data_npz = np.load(args[\"data_path\"])\n",
    "\n",
    "if args[\"model\"] == \"LeNet\":\n",
    "    transform = Compose([ToTensor(), Resize((32,32), antialias=False)])\n",
    "elif args[\"model\"] == \"ResNet\":\n",
    "    raise NotImplementedError\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "dataset_train = CharDataset(data_npz[\"x_train\"], data_npz[\"y_train\"], transform)\n",
    "dataloader_train = DataLoader(dataset_train, args[\"batch_size\"], shuffle=True)\n",
    "\n",
    "dataset_valid = CharDataset(data_npz[\"x_valid\"], data_npz[\"y_valid\"], transform)\n",
    "dataloader_valid = DataLoader(dataset_valid, args[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"model\"] == \"LeNet\":\n",
    "    model = LeNet(1, 12)\n",
    "elif args[\"model\"] == \"ResNet\":\n",
    "    raise NotImplementedError\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "classifier = CNNClassifier(model, gpu=0)\n",
    "if os.path.exists(args[\"save_path\"]):\n",
    "    classifier.load_model(args[\"save_path\"])\n",
    "    print(classifier.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"mode\"] == \"train\" or args[\"mode\"] == \"train_and_test\":\n",
    "    classifier.fit(\n",
    "        train_loader=dataloader_train,\n",
    "        valid_loader=dataloader_valid,\n",
    "        epoches=args[\"epoches\"],\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        save_path=args[\"save_path\"],\n",
    "        log_interval=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"mode\"] != \"test\" and args[\"mode\"] != \"train_and_test\":\n",
    "    exit(0)\n",
    "\n",
    "raw_test_path = \"\"\n",
    "test_path = \"\"\n",
    "# if not os.path.exists(test_path):\n",
    "#     test_preprocess(raw_test_path, test_path, shuffle=True)\n",
    "# test_npz = np.load(test_path)\n",
    "\n",
    "# dataset_test = CharDataset(test_npz[\"x_test\"], test_npz[\"y_test\"], transform)\n",
    "# dataloader_test = DataLoader(dataset_test, args[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# loss = 0.0\n",
    "# acc = 0.0\n",
    "# loss_func = CrossEntropyLoss()\n",
    "# for batch in dataloader_test:\n",
    "#     result = classifier.predict(batch[\"x\"])\n",
    "#     loss += loss_func(result[\"predicts\"], batch[\"y\"])\n",
    "#     acc += accuracy(result[\"predicts\"], batch[\"y\"])\n",
    "# loss /= len(dataloader_test)\n",
    "# acc /= len(dataloader_test)\n",
    "# logging.info(\"Test mean loss: {:.6f}, mean accuracy: {:.6f}\".format(loss, acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73f83993ecc2dce571ce89cddd8a44e114591bb14d9f1f8465ac4f80026585cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('AI_A')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
